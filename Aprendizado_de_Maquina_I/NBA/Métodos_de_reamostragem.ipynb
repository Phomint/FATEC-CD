{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Métodos_de_reamostragem.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"AWL82iEuR_JO"},"source":["## Métodos de reamostragem\n","\n","### Créditos\n","\n","1. http://cursos.leg.ufpr.br/ML4all/apoio/reamostragem.html\n","\n","2. https://dcm.ffclrp.usp.br/~augusto/teaching/ami/AM-I-Metodos-Amostragem.pdf\n","\n","\n","### Conteúdo\n","\n"," - Holdout\n"," - Cross-Validation (also known as k-Fold Cross Validation)\n","  - Stratified Cross-Validation\n","  - Leave-One-Out\n"," - Bootstrap"]},{"cell_type":"markdown","metadata":{"id":"qgLjtJ8lYVVM"},"source":["Métodos de reamostragem são ferramentas indispensáveis na estatística moderna. As técnicas envolvem particionar os dados de treino e reajustar os modelos em competição para cada subamostra, a fim de obter informações adicionais sobre o ajuste do modelo (que não seria possível com os dados completos)."]},{"cell_type":"markdown","metadata":{"id":"KlaSXQ7Z6K0k"},"source":["**Carregar dados**"]},{"cell_type":"code","metadata":{"id":"TKNK8MtvDOI4","executionInfo":{"status":"ok","timestamp":1619224456023,"user_tz":180,"elapsed":2392,"user":{"displayName":"Robson Parmezan Bonidia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKl6Ts6LeXfFuSTEY7AffHsBmlvFkJt_vN15qM=s64","userId":"11900109175221565994"}}},"source":["from sklearn.datasets import load_breast_cancer\n","\n","data = load_breast_cancer()\n","X = data.data\n","y = data.target"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aBszVTRlYfjX"},"source":["**Validação Holdout**\n","\n","A atividade envolve dividir aleatoriamente o conjunto de dados em (i) treinamento: utilizado para preparar o modelo (treiná-lo) e (ii) validação: utilizado para avaliar o desempenho do modelo treinado."]},{"cell_type":"markdown","metadata":{"id":"c6fKK9iycjpK"},"source":["<img src=\"https://algotrading101.com/learn/wp-content/uploads/2020/06/training-validation-test-data-set.png\" alt=\"drawing\" width=\"600\"/>\n","\n","Source: https://algotrading101.com/learn/train-test-split/"]},{"cell_type":"code","metadata":{"id":"iJTx1aj56ZVy","executionInfo":{"status":"ok","timestamp":1619224871351,"user_tz":180,"elapsed":715,"user":{"displayName":"Robson Parmezan Bonidia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKl6Ts6LeXfFuSTEY7AffHsBmlvFkJt_vN15qM=s64","userId":"11900109175221565994"}}},"source":["from sklearn.model_selection import train_test_split\n","\n","train, test, train_labels, test_labels = train_test_split(X,\n","                                                          y,\n","                                                          test_size=0.3,\n","                                                          random_state=12,\n","                                                          stratify=y)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WgXNURzOUtK0","executionInfo":{"status":"ok","timestamp":1619224943903,"user_tz":180,"elapsed":535,"user":{"displayName":"Robson Parmezan Bonidia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKl6Ts6LeXfFuSTEY7AffHsBmlvFkJt_vN15qM=s64","userId":"11900109175221565994"}},"outputId":"a9ed8e5c-e56d-4f5e-fe72-98fa14c700a5"},"source":["test.shape\n","train.shape"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(398, 30)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HDtmO6bkU4T1","executionInfo":{"status":"ok","timestamp":1619224931390,"user_tz":180,"elapsed":565,"user":{"displayName":"Robson Parmezan Bonidia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKl6Ts6LeXfFuSTEY7AffHsBmlvFkJt_vN15qM=s64","userId":"11900109175221565994"}},"outputId":"291c2538-b70d-40ec-bfd4-2985d6c28675"},"source":["X.shape"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(569, 30)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"3Gnavv1N7DQR"},"source":["**Cross-Validation**\n","\n","Separar os dados em somente duas partes disjuntas pode trazer resultados divergentes, dependendo da informação contida em cada conjunto (especialmente quando os dados são escassos). A abordagem de validação cruzada por k-fold minimiza esses problemas. O método consiste em dividir os dados em K partes iguais, ajusta-se o modelo utilizando K−1 partes, e a parcela restante fica destinada à validação. Esse processo é repetido K vezes (em cada momento uma partição diferente será a validação), em seguida os resultados são combinados obtendo a médias dos erros obtidos."]},{"cell_type":"markdown","metadata":{"id":"xFw1KndycvGY"},"source":["<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" alt=\"drawing\" width=\"600\"/>\n","\n","Source: https://scikit-learn.org/stable/modules/cross_validation.html"]},{"cell_type":"code","metadata":{"id":"HZ98wR7g5WW-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619225609309,"user_tz":180,"elapsed":3056,"user":{"displayName":"Robson Parmezan Bonidia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKl6Ts6LeXfFuSTEY7AffHsBmlvFkJt_vN15qM=s64","userId":"11900109175221565994"}},"outputId":"834ebec2-49c8-4516-e20b-a9b654bd3ddf"},"source":["from sklearn.pipeline import Pipeline\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import cross_val_score\n","from sklearn.preprocessing import StandardScaler\n","\n","data = load_breast_cancer()\n","X = data.data\n","y = data.target\n","\n","pipeline = Pipeline(steps=[\n","    ('scaler', StandardScaler()),\n","    ('classifier', RandomForestClassifier())])\n","\n","cv_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv = 10)\n","\n","print(\"Cross-val accuracy: %f\" % cv_scores.mean())"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Cross-val accuracy: 0.964912\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KmwAm00Z7MVy"},"source":["**Stratified Cross-Validation**\n","\n","O estimador stratified cross-validation é similar à cross-validation, mas ao gerar os folds mutuamente exclusivos, a distribuição de classe (proporção de exemplos em cada uma das classes) é considerada durante a amostragem. Isto significa, por exemplo, que se o conjunto original de exemplos possui duas classes com distribuição de 20% e 80%, então cada fold também terá esta proporção de classes\n"]},{"cell_type":"markdown","metadata":{"id":"P72tt1LDb0Q9"},"source":["<img src=\"https://dataaspirant.com/wp-content/uploads/2020/12/8-Stratified-K-Fold-Cross-Validation.png\" alt=\"drawing\" width=\"600\"/>\n","\n","Source: https://dataaspirant.com/8-stratified-k-fold-cross-validation/\n","\n"]},{"cell_type":"code","metadata":{"id":"DhQIO8Cu7czY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619225754747,"user_tz":180,"elapsed":2800,"user":{"displayName":"Robson Parmezan Bonidia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKl6Ts6LeXfFuSTEY7AffHsBmlvFkJt_vN15qM=s64","userId":"11900109175221565994"}},"outputId":"2fa92bb3-5bea-4546-fea0-01bac883f2e3"},"source":["import numpy as np\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import accuracy_score\n","\n","folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=20)\n","score = []\n","for train_index, test_index in folds.split(X, y):\n","    X_train, X_val = X[train_index], X[test_index]\n","    y_train, y_val = y[train_index], y[test_index]\n","    model = RandomForestClassifier()\n","    model.fit(X_train, y_train)\n","    score.append(accuracy_score(y_val, model.predict(X_val)))\n","np.mean(score) "],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9613408521303258"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"WnKH_ZbP7Tga"},"source":["**Leave-One-Out**\n","\n","O estimador leave-one-out é um caso especial de cross-validation. É computacionalmente dispendioso e freqüentemente é usado em amostras pequenas. Para uma amostra de tamanho n uma hipótese é induzida utilizando (n-1) exemplos; a hipótese é então testada no único exemplo remanescente. Este processo é repetido n vezes, cada vez induzindo uma hipótese deixando de considerar um único exemplo."]},{"cell_type":"markdown","metadata":{"id":"H4lMoSD0a-ke"},"source":["<img src=\"https://assets.datacamp.com/production/repositories/3981/datasets/8a6236f142b1ee2e4a70aae2af9507c7c580f302/Screen%20Shot%202019-01-27%20at%209.25.41%20AM.png\" alt=\"drawing\" width=\"600\"/>\n","\n","Source: https://campus.datacamp.com/courses/model-validation-in-python/cross-validation?ex=10"]},{"cell_type":"code","metadata":{"id":"hcW0Jd2k9Hi7","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"70414fce-1d03-4a8d-c5d4-4f099e9f2f62"},"source":["from sklearn.model_selection import LeaveOneOut\n","\n","loo = LeaveOneOut()\n","score = []\n","for train_index, test_index in loo.split(X, y):\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","    model = RandomForestClassifier()\n","    model.fit(X_train, y_train)\n","    score.append(accuracy_score(y_test, model.predict(X_test)))\n","print(np.mean(score))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.9560632688927944\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AmAf09N394aC","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"58388054-346d-44f7-9515-789bc37b6497"},"source":["len(score)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["569"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"DIER7PIU7VVD"},"source":["**Bootstrap**\n","\n","No estimador bootstrap, a idéia básica consiste em repetir o processo de classificação um grande número de vezes. Estima-se então valores, tais como o erro ou bias, a partir dos experimentos replicados, cada experimento sendo conduzido com base em um novo conjunto de treinamento obtido por amostragem com reposição do conjunto original de exemplos"]},{"cell_type":"markdown","metadata":{"id":"JCw5V3qraWXe"},"source":["![](http://rasbt.github.io/mlxtend/user_guide/evaluate/BootstrapOutOfBag_files/bootstrap_concept.png)\n","\n","Source: http://rasbt.github.io/mlxtend/user_guide/evaluate/bootstrap_point632_score/"]},{"cell_type":"code","metadata":{"id":"QcvJImGM-c0i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619226609628,"user_tz":180,"elapsed":213279,"user":{"displayName":"Robson Parmezan Bonidia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKl6Ts6LeXfFuSTEY7AffHsBmlvFkJt_vN15qM=s64","userId":"11900109175221565994"}},"outputId":"9c4ad4b5-ac69-45dd-9dca-c1bc2a8485d2"},"source":["from sklearn.model_selection import ShuffleSplit\n","\n","score = []\n","ss = ShuffleSplit(n_splits=1000, test_size=0.25, random_state=3)\n","for train_index, test_index in ss.split(X, y):\n","    # print(\"%s %s\" % (train_index, test_index))\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","    model = RandomForestClassifier()\n","    model.fit(X_train, y_train)\n","    score.append(accuracy_score(y_test, model.predict(X_test)))\n","print(np.mean(score))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["0.9586923076923076\n"],"name":"stdout"}]}]}